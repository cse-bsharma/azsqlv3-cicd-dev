{
	"name": "serverless demo",
	"properties": {
		"content": {
			"query": "-- COVID data set\n-- In this sample is used the latest available public data on geographic distribution of COVID-19 cases worldwide \n-- from the European Center for Disease Prevention and Control (ECDC). Each row/entry contains the number of new \n-- cases reported per day and per country. For more information about this dataset, see here. Data set is updated on \n-- daily basis and placed as a part of Azure Open Dataset.\n\n\n--The OPENROWSET function reads content of a remote data source (for example file) and returns the content as a set of rows.\n--The OPENROWSET function will give you information about the columns in the external files or containers and enable you to\n--define a schema of your external tables and views.\n\nSELECT TOP 100 * FROM\n    OPENROWSET(\n        BULK 'https://pandemicdatalake.blob.core.windows.net/public/curated/covid-19/ecdc_cases/latest/ecdc_cases.parquet',\n        FORMAT='PARQUET'\n    ) AS [result]\n\n-- Configuring data sources and formats\n-- As a first step you need to configure data source and specify file format of remotely stored files.\n--drop external  DATA SOURCE ecdc_cases \nCREATE EXTERNAL DATA SOURCE ecdc_cases WITH (\n    LOCATION = 'https://pandemicdatalake.blob.core.windows.net/public/curated/covid-19/ecdc_cases/'\n);\n\nCREATE EXTERNAL FILE FORMAT ParquetFormat WITH (  FORMAT_TYPE = PARQUET );\n\n--First statement creates data source that references ECDC COVID data set, while the second specifies parquet file format.\n\n--Exploring file schema\n--Now you need to determine what are the columns in the external files and what are their types\n--In the previous example you might have seen that OPENROWSET function enables you to quickly explore data in the files placed on Azure storage.\nexec sp_describe_first_result_set @tsql= N'SELECT TOP 0 * FROM\n    OPENROWSET(\n        BULK ''https://pandemicdatalake.blob.core.windows.net/public/curated/covid-19/ecdc_cases/latest/ecdc_cases.parquet'',\n        FORMAT=''PARQUET''\n    ) AS [result]'\n\n--Creating an external table\n\n--create a schema\ncreate schema demo;\n\n--create external table\n--drop external table demo.cases \ncreate external table demo.cases (\n    date_rep                   date,\n    day                        smallint,\n    month                      smallint,\n    year                       smallint,\n    cases                      smallint,\n    deaths                     smallint,\n    countries_and_territories  varchar(256),\n    geo_id                     varchar(60),\n    country_territory_code     varchar(16),\n    pop_data_2018              int,\n    continent_exp              varchar(32),\n    load_date                  datetime2(7),\n    iso_country                varchar(16)\n) with (\n    location = 'latest/ecdc_cases.parquet',\n    data_source= ecdc_cases,\n    file_format = ParquetFormat\n);\n\n\n\n\nselect top 10  *\nfrom openrowset(bulk 'latest/ecdc_cases.parquet',\n                data_source = 'ecdc_case',\n                format='parquet') as a            \n\n\n--Query external table\n\nselect top(10) * from demo.cases \n\n--create a logical view\n\ncreate or alter view demo.vw_cases\nas select year(date_rep) as [year], sum(deaths) as total_cases,\ncountries_and_territories from demo.cases group by year(date_rep) ,countries_and_territories  having  sum(cases) > 1000000;\n\n--Query and vislaize the view\n\nselect * from demo.vw_cases\n\n\n--Access and permissions\nCREATE USER [mubhashk@microsoft.com] FROM EXTERNAL PROVIDER;\nGO\nDENY ADMINISTER DATABASE BULK OPERATIONS TO [mubhashk@microsoft.com]\nGO\nGRANT SELECT ON SCHEMA::demo TO [mubhashk@microsoft.com]\nGO\nGRANT SELECT ON OBJECT::demo.vw_cases TO [mubhashk@microsoft.com]\nGO\nGRANT REFERENCES ON DATABASE SCOPED CREDENTIAL::MyCosmosDbAccountCredential TO [mubhashk@microsoft.com]\nGO\n\n\n--Delta\n-- The easiest way to see to the content of your DELTA file is to provide the file URL\n--  to the OPENROWSET function and specify DELTA format.\n\n-- Column names and data types are automatically read from Delta Lake files. The OPENROWSET function\n--  uses best guess types like VARCHAR(1000) for the string columns.\n\n\nSELECT TOP 10 *\nFROM OPENROWSET(\n    BULK 'https://sqlondemandstorage.blob.core.windows.net/delta-lake/covid/',\n    FORMAT = 'delta') as rows;\n\n\nCREATE EXTERNAL DATA SOURCE DeltaLakeStorage\nWITH ( LOCATION = 'https://sqlondemandstorage.blob.core.windows.net/delta-lake/' );\nGO\n\nSELECT TOP 10 *\nFROM OPENROWSET(\n        BULK 'covid',\n        DATA_SOURCE = 'DeltaLakeStorage',\n        FORMAT = 'delta'\n    ) as rows;\n\n    -- Explicitly specify schema\n\n    SELECT TOP 10 *\nFROM OPENROWSET(\n        BULK 'covid',\n        DATA_SOURCE = 'DeltaLakeStorage',\n        FORMAT = 'delta'\n    )\n    WITH ( date_rep date,\n           cases int,\n           geo_id varchar(6)\n           ) as rows;\n\n-- Conclusion\n-- External tables in Azure Synapse SQL query engine represent logical relational adapter created on top of externally stored\n--  files that can be used by any application that use TSQL to query data. This way you can build a Logical Data Warehouse on \n--  top of your data stored in Azure Data Lake without need to load data in standard relational table. Azure Synapse SQL \n--  Logical Data Warehouse enables you to represent external data sources as standard tables and let you analytic/reporting\n--   applications access any data without need to know where the data is place and hot to parse the data structure.\n\n\n",
			"metadata": {
				"language": "sql"
			},
			"currentConnection": {
				"databaseName": "test",
				"poolName": "Built-in"
			},
			"resultLimit": 5000
		},
		"type": "SqlQuery"
	}
}