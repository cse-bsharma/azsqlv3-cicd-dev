{
	"name": "adxnotebook",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkload",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "10e6741b-37ba-4cb6-8bc9-d70cc343f1b7"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/7e416de3-c506-4776-8270-83fd73c6cc37/resourceGroups/cse-dw-pilot/providers/Microsoft.Synapse/workspaces/azsqlv3/bigDataPools/sparkload",
				"name": "sparkload",
				"type": "Spark",
				"endpoint": "https://azsqlv3.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkload",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 10,
				"cores": 16,
				"memory": 112,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql.types import *\r\n",
					"from pyspark.sql import Row\r\n",
					"from pyspark.sql.types import StructField, StructType, StringType, LongType"
				],
				"attachments": null,
				"execution_count": 60
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"account_name = \"azdwsynapse\"\r\n",
					"container_name= \"adx\"\r\n",
					"relative_path = \"ship.json\"\r\n",
					"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name,relative_path)\r\n",
					"print(adls_path)\r\n",
					"df=spark.read.format(\"json\").option(\"inferSchema\", \"true\").load(adls_path)\r\n",
					"df=df.withColumnRenamed(\"ns:ShpPcdDetailsMsg\", \"ShpPcdDetailsMsg\")\r\n",
					""
				],
				"attachments": null,
				"execution_count": 61
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import explode\r\n",
					"df.withColumnRenamed(\"ns:ShpPcdDetailsMsg.Bd.Shp.ShpKey\", \"shpkey\")\\\r\n",
					".withColumn(\"customer\",explode(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust.CustRolTyCd\")).\\\r\n",
					"select(\"ns:ShpPcdDetailsMsg.Bd.Shp.ShpKey\",\"customer\").count()"
				],
				"attachments": null,
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df.printSchema()"
				],
				"attachments": null,
				"execution_count": 62
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"complex_fields = dict([(field.name, field.dataType) for field in df.schema.fields if type(field.dataType) == ArrayType or \\\r\n",
					"type(field.dataType) == StructType])\r\n",
					"col_name=list(complex_fields.keys())[0]\r\n",
					"print(type(col_name))\r\n",
					"col_name = col_name.split(\":\", 1)\r\n",
					"print(type(col_name))\r\n",
					"print(str(col_name[1]))"
				],
				"attachments": null,
				"execution_count": 43
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.types import *\r\n",
					"from pyspark.sql.functions import *\r\n",
					"\r\n",
					"#Flatten array of structs and structs\r\n",
					"def flatten(df):\r\n",
					"   # compute Complex Fields (Lists and Structs) in Schema   \r\n",
					"   complex_fields = dict([(field.name, field.dataType)\r\n",
					"                             for field in df.schema.fields\r\n",
					"                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\r\n",
					"   while len(complex_fields)!=0:\r\n",
					"      col_name=list(complex_fields.keys())[0]\r\n",
					"      print (\"Processing :\"+col_name+\" Type : \"+str(type(complex_fields[col_name])))\r\n",
					"    \r\n",
					"      # if StructType then convert all sub element to columns.\r\n",
					"      # i.e. flatten structs\r\n",
					"      if (type(complex_fields[col_name]) == StructType):\r\n",
					"         expanded = [col(col_name+'.'+k).alias(col_name+'_'+k) for k in [ n.name for n in  complex_fields[col_name]]]\r\n",
					"         df=df.select(\"*\", *expanded).drop(col_name)\r\n",
					"    \r\n",
					"      # if ArrayType then add the Array Elements as Rows using the explode function\r\n",
					"      # i.e. explode Arrays\r\n",
					"      elif (type(complex_fields[col_name]) == ArrayType):    \r\n",
					"         df=df.withColumn(col_name,explode_outer(col_name))\r\n",
					"    \r\n",
					"      # recompute remaining Complex Fields in Schema       \r\n",
					"      complex_fields = dict([(field.name, field.dataType)\r\n",
					"                             for field in df.schema.fields\r\n",
					"                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\r\n",
					"   return df\r\n",
					"\r\n",
					"df=flatten(df)\r\n",
					"df.printSchema()"
				],
				"attachments": null,
				"execution_count": 49
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df=df.withColumnRenamed(\"ShpPcdDetailsMsg_Bd_Shp_ActvDtm_#text\", \"ShpPcdDetailsMsg_Bd_Shp_ActvDtm_text\")\\\r\n",
					".withColumnRenamed(\"ShpPcdDetailsMsg_Bd_Shp_Dsc_#text\", \"ShpPcdDetailsMsg_Bd_Shp_Dsc_text\")\\\r\n",
					".withColumnRenamed(\"ShpPcdDetailsMsg_Bd_Shp_GlProdSrvCd_#text\", \"ShpPcdDetailsMsg_Bd_Shp_GlProdSrvCd_text\")\\\r\n",
					".withColumnRenamed(\"ShpPcdDetailsMsg_Bd_Shp_LclProdSrvCd_#text\", \"ShpPcdDetailsMsg_Bd_Shp_LclProdSrvCd_text\")\\\r\n",
					".withColumnRenamed(\"ShpPcdDetailsMsg_Bd_Shp_PlnDtm_#text\", \"ShpPcdDetailsMsg_Bd_Shp_PlnDtm_text\")\\\r\n",
					".withColumnRenamed(\"ShpPcdDetailsMsg_Bd_Shp_Amt_Amt_#text\", \"ShpPcdDetailsMsg_Bd_Shp_Amt_Amt_text\")\\\r\n",
					".withColumnRenamed(\"ShpPcdDetailsMsg_Bd_Shp_ShpDoc_Dtm_#text\", \"ShpPcdDetailsMsg_Bd_Shp_ShpDoc_Dtm_text\")"
				],
				"attachments": null,
				"execution_count": 57
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df.createOrReplaceTempView(\"shipment\")"
				],
				"attachments": null,
				"execution_count": 50
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.sql(\"select `ShpPcdDetailsMsg_Bd_Shp_ShpKey` from shipment \\\r\n",
					"where `ShpPcdDetailsMsg_Bd_Shp_Cust_CustRolTyCd` = 'RV' and \\\r\n",
					"`ShpPcdDetailsMsg_Bd_Shp_Cust_Addr_ActPntSp_CtryCd` = 'SG'\\\r\n",
					"and `ShpPcdDetailsMsg_Bd_Shp_Cust_Addr_ActPntSp_CtryCd` in ('Q','C')\").show()"
				],
				"attachments": null,
				"execution_count": 59
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.sql(\"select distinct `ShpPcdDetailsMsg_Bd_Shp_Cust_CustRolTyCd` from shipment\").show()"
				],
				"attachments": null,
				"execution_count": 53
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.sql(\"select count(*) from shipment\").show()"
				],
				"attachments": null,
				"execution_count": 54
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"account_name = \"azdwsynapse\"\r\n",
					"container_name= \"adx\"\r\n",
					"relative_path = \"parquet\"\r\n",
					"adls_path_parquet = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name,relative_path)\r\n",
					"df.write.format(\"parquet\").save(adls_path_parquet+\"/ship\")"
				],
				"attachments": null,
				"execution_count": 58
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# from pyspark.sql.functions import explode\r\n",
					"# from pyspark.sql.functions import get_json_object, json_tuple\r\n",
					"# #df.printSchema()\r\n",
					"# #df.select(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust\").printSchema()\r\n",
					"# # df.select(explode(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust\").alias(\"customer\")).select(\"customer.*\").show(10)\r\n",
					"# #df.select(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust.Addr.ActPntSp.CtryCd\").show(10)\r\n",
					"# #df.where(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust.CustRolTyCd = RV\").show(10)\r\n",
					"# #explode(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust.Addr.ActPntSp.CtryCd\").alias(\"countrycode\")\r\n",
					"# df.withColumnRenamed(\"ns:ShpPcdDetailsMsg.Bd.Shp.ShpKey\", \"shpkey\").withColumn(\"countrycode\", \\\r\n",
					"# explode(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust.Addr.ActPntSp.CtryCd\")) \\\r\n",
					"# .withColumn(\"customer\",explode(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust.CustRolTyCd\")).\\\r\n",
					"# select(\"ns:ShpPcdDetailsMsg.Bd.Shp.ShpKey\",\"customer\",\"countrycode\").show(20,False)\r\n",
					"# #df.withColumn(\"countrycode\",explode(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust.Addr.ActPntSp.CtryCd\")).withColumn(\"customer\",explode(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust.CustRolTyCd\")).show(10)\r\n",
					"# # df.select(explode(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust\").alias(\"customer\")).show(2)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# new_df=df.select(\"ns:ShpPcdDetailsMsg.Bd.Shp.ShpKey\",\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust.Addr.ActPntSp.CtryCd\",\\\r\n",
					"# \"ns:ShpPcdDetailsMsg.Bd.Shp.Cust.NtwAccNo\",\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust.CustRolTyCd\",\\\r\n",
					"# \"ns:ShpPcdDetailsMsg.Bd.Shp.GlProdSrvCd.GlProdSrvClssCd\"\\\r\n",
					"# ,\"ns:ShpPcdDetailsMsg.Bd.Shp.GlProdSrvCd.#text\")\r\n",
					"# # df.select(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust.CustRolTyCd\").show(10)\r\n",
					"# # from pyspark.sql.functions import *\r\n",
					"# # import json\r\n",
					"# df.select(col(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust\").getItem(0)).printSchema()\r\n",
					"# # df.to_json(orient='index')"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# new_df.printSchema()"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# from pyspark.sql.types import *\r\n",
					"# from pyspark.sql.functions import *\r\n",
					"\r\n",
					"# #Flatten array of structs and structs\r\n",
					"# def flatten(df):\r\n",
					"#    # compute Complex Fields (Lists and Structs) in Schema   \r\n",
					"#    complex_fields = dict([(field.name, field.dataType)\r\n",
					"#                              for field in df.schema.fields\r\n",
					"#                              if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\r\n",
					"#    while len(complex_fields)!=0:\r\n",
					"#       col_name=list(complex_fields.keys())[0]\r\n",
					"#       print (\"Processing :\"+col_name+\" Type : \"+str(type(complex_fields[col_name])))\r\n",
					"    \r\n",
					"#       # if StructType then convert all sub element to columns.\r\n",
					"#       # i.e. flatten structs\r\n",
					"#       if (type(complex_fields[col_name]) == StructType):\r\n",
					"#          expanded = [col(col_name+'.'+k).alias(col_name+'_'+k) for k in [ n.name for n in  complex_fields[col_name]]]\r\n",
					"#          df=df.select(\"*\", *expanded).drop(col_name)\r\n",
					"    \r\n",
					"#       # if ArrayType then add the Array Elements as Rows using the explode function\r\n",
					"#       # i.e. explode Arrays\r\n",
					"#       elif (type(complex_fields[col_name]) == ArrayType):    \r\n",
					"#          df=df.withColumn(col_name,explode_outer(col_name))\r\n",
					"    \r\n",
					"#       # recompute remaining Complex Fields in Schema       \r\n",
					"#       complex_fields = dict([(field.name, field.dataType)\r\n",
					"#                              for field in df.schema.fields\r\n",
					"#                              if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\r\n",
					"#    return df\r\n",
					"\r\n",
					"# df=flatten(df)\r\n",
					"# df.printSchema()"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# df.createOrReplaceTempView(\"shipment\")"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# spark.sql(\"select `ns:ShpPcdDetailsMsg_Bd_Shp_ShpKey` from shipment \\\r\n",
					"# where `ns:ShpPcdDetailsMsg_Bd_Shp_Cust_CustRolTyCd` = 'RV' and \\\r\n",
					"# `ns:ShpPcdDetailsMsg_Bd_Shp_Cust_Addr_ActPntSp_CtryCd` = 'SG'\\\r\n",
					"# and `ns:ShpPcdDetailsMsg_Bd_Shp_Cust_Addr_ActPntSp_CtryCd` in ('Q','C')\").show()"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# from pyspark.sql.functions import explode\r\n",
					"# from pyspark.sql.functions import get_json_object, json_tuple\r\n",
					"# #df.printSchema()\r\n",
					"# #df.select(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust\").printSchema()\r\n",
					"# df.select(explode(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust\").alias(\"customer\")).select(\"customer.*\").show(10)\r\n",
					"# #df.select(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust.Addr.ActPntSp.CtryCd\").show(10)\r\n",
					"# #df.where(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust.CustRolTyCd = RV\").show(10)\r\n",
					"# #explode(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust.Addr.ActPntSp.CtryCd\").alias(\"countrycode\")\r\n",
					"# # df.withColumnRenamed(\"ns:ShpPcdDetailsMsg.Bd.Shp.ShpKey\", \"shpkey\").withColumn(\"countrycode\", \\\r\n",
					"# # explode(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust.Addr.ActPntSp.CtryCd\")) \\\r\n",
					"# # .withColumn(\"customer\",explode(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust.CustRolTyCd\")).\\\r\n",
					"# # select(\"ns:ShpPcdDetailsMsg.Bd.Shp.ShpKey\",\"customer\",\"countrycode\").show(20,False)\r\n",
					"# #df.withColumn(\"countrycode\",explode(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust.Addr.ActPntSp.CtryCd\")).withColumn(\"customer\",explode(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust.CustRolTyCd\")).show(10)\r\n",
					"# # df.select(explode(\"ns:ShpPcdDetailsMsg.Bd.Shp.Cust\").alias(\"customer\")).show(2)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# df.createOrReplaceTempView(\"shipment\")"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# spark.sql(\"select from shipment LATERAL VIEW explode(ns:ShpPcdDetailsMsg.Bd.Shp.Cust.Addr.ActPntSp.CtryCd) as col\").show()"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# from pyspark.sql.functions import *\r\n",
					"# jsonDF = spark.range(1).selectExpr(\"\"\"'{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")\r\n",
					"# from pyspark.sql.functions import get_json_object, json_tuple\r\n",
					"# jsonDF.select(get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\") ,json_tuple(col(\"jsonString\"), \"myJSONKey\")).show(2)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# account_name = \"azdwsynapse\"\r\n",
					"# container_name= \"adx\"\r\n",
					"# relative_path = \"parquet\"\r\n",
					"# adls_path_parquet = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name,relative_path)\r\n",
					"# df.select(\"Body\").write.format(\"parquet\").save(adls_path_parquet+\"/test\")"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# df_parquet=spark.read.format(\"parquet\").option(\"inferSchema\", \"true\").load(adls_path_parquet+\"/test\")"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# df_parquet.printSchema()"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# from pyspark.sql.functions import *\r\n",
					"# df = df_parquet.withColumn(\"Body\", col(\"Body\").cast(\"string\"))"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# df.show()"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# import json\r\n",
					"# #body string looks like json\r\n",
					"# dfBody = df.select(df.Body)\r\n",
					"# jsonList = (dfBody.collect())\r\n",
					"# jsonString = jsonList[0][0]"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# newdf=df.select(col(\"Body\"), substring_index(col(\"Body\"), \"|\", -1).alias(\"json_body\"))"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# newdf.show()"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# import json\r\n",
					"# results = json.loads(newdf.select(\"json_body\").toJSON().first())"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# for key in results:\r\n",
					"#         print (results[key])"
				],
				"attachments": null,
				"execution_count": 64
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# def print_rows(row):\r\n",
					"#     data = json.loads(row)\r\n",
					"#     for key in data:\r\n",
					"#         print (\"{key}:{value}\".format(key=key, value=data[key]))"
				],
				"attachments": null,
				"execution_count": 65
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# results.foreach(print_rows)"
				],
				"attachments": null,
				"execution_count": 68
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# account_name = \"azdwsynapse\"\r\n",
					"# container_name= \"adx\"\r\n",
					"# relative_path = \"parquet\"\r\n",
					"# adls_path_parquet = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name,relative_path)\r\n",
					"# newdf.select(\"json_body\").write.format(\"json\").save(adls_path_parquet+\"/json\")"
				],
				"attachments": null,
				"execution_count": 69
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# df_json=spark.read.format(\"json\").option(\"inferSchema\", \"true\").load(adls_path_parquet+\"/json\")"
				],
				"attachments": null,
				"execution_count": 70
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# result=df_json.toJSON()"
				],
				"attachments": null,
				"execution_count": 75
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# def print_rows(row):\r\n",
					"#     data = json.loads(row)\r\n",
					"#     for key in data:\r\n",
					"#         print (\"{key}:{value}\".format(key=key, value=data[key]))"
				],
				"attachments": null,
				"execution_count": 77
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# result.foreach(print_rows)"
				],
				"attachments": null,
				"execution_count": 80
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# type(result)\r\n",
					"# result.saveAsTextFile(\"abfss://adx@azdwsynapse.dfs.core.windows.net/test.json\")"
				],
				"attachments": null,
				"execution_count": 85
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# df_json_new=spark.read.format(\"json\").option(\"inferSchema\", \"true\").load(\"abfss://adx@azdwsynapse.dfs.core.windows.net/test.json\")"
				],
				"attachments": null,
				"execution_count": 86
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# df_json_new.printSchema()"
				],
				"attachments": null,
				"execution_count": 88
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# // %%spark\r\n",
					"# // val account_name = \"azdwsynapse\"\r\n",
					"# // val container_name= \"adx\"\r\n",
					"# // val relative_path = \"test.avro\"\r\n",
					"# // val adls_path = \"abfss://adx@azdwsynapse.dfs.core.windows.net/test.avro\"\r\n",
					"# // val df = spark.read.format(\"avro\").load(adls_path)\r\n",
					"# // val bodyRDD = df.select(col(\"Body\").cast(\"string\")).rdd.map(x=>x(0).toString())\r\n",
					"# // val data = spark.read.json(bodyRDD)\r\n",
					"# // data.show()"
				],
				"attachments": null,
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# %%pyspark\n",
					"\n",
					"# # Read data from Azure Data Explorer table(s)\n",
					"# # Full Sample Code available at: https://github.com/Azure/azure-kusto-spark/blob/master/samples/src/main/python/SynapseSample.py\n",
					"\n",
					"# kustoDf  = spark.read \\\n",
					"#     .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
					"#     .option(\"spark.synapse.linkedService\", \"AzureDataExplorer\") \\\n",
					"#     .option(\"kustoDatabase\", \"adxexplorer\") \\\n",
					"#     .option(\"kustoQuery\", \"nyctaxi | take 10\") \\\n",
					"#     .load()\n",
					"\n",
					"# display(kustoDf)\n",
					""
				],
				"attachments": null,
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# df.write \\\r\n",
					"#     .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\r\n",
					"#     .option(\"spark.synapse.linkedService\", \"<link service name>\") \\\r\n",
					"#     .option(\"kustoDatabase\", \"<Database name>\") \\\r\n",
					"#     .option(\"kustoTable\", \"<Table name>\") \\\r\n",
					"#     .mode(\"Append\") \\\r\n",
					"#     .save()"
				],
				"attachments": null,
				"execution_count": null
			}
		]
	}
}